---
title: "Homework3"
author: "xueyurong"
date: "2020年3月27日"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```
#Approximation of the distribution function by the Monte Carlo methods

##1.Introduction

  Consider approximation of the distribution function of
$N(0, 1)$,
\begin{equation}
\Phi(t) = \int_{-\infty}^t \frac{1}{\sqrt{2\pi}} e^{-y^2 / 2} {\rm d} y,
(\#eq:cdf)
\end{equation}
by the Monte Carlo methods:
\begin{equation}
\hat\Phi(t) = \frac{1}{n} \sum_{i=1}^n I(X_i \le t),
\end{equation}
where $X_i$'s are a random sample from $N(0, 1)$, and $I(\cdot)$ is
the indicator function. Experiment with the approximation at
$n \in \{10^2, 10^3, 10^4\}$ at
$t \in \{0.0, 0.67, 0.84, 1.28, 1.65, 2.32, 2.58, 3.09, 3.72\}$ to
form a table.

##2.Monte Carlo method
 
   Monte Carlo method,also known as statistical simulation method.It refers to the use of random numbers (or more common pseudo-random numbers) to solve many calculation problems.
   
   When the problem to be solved is the probability of occurrence of a random event, or the expected value of a random variable, the probability of this random event can be estimated by the frequency of occurrence of this event, or some numerical characteristics of this random variable can be obtained, and it can be regarded as the solution of the problem.
   
   The problem solving process of Monte Carlo method can be summarized into three main steps:
 
 (1) Construct or describe probability process;
 
 (2) Realize sampling from known probability distribution; 
 
 (3) Establish various estimators.
 
##3.Result

####3.1 Calculate the truevalue,and form the table.

```{r,echo=FALSE}
  t=c(0,0.67,0.84,1.28,1.65,2.32,2.58,3.09,3.72)
  (truevalue=pnorm(t))
#n=100时
n1=100
x1=c(0,0,0,0,0,0,0,0,0)
y1=c(rnorm(n1,mean=0,sd=1))
for(i in 1:9){
  a=0
  for(j in 1:n1){
    if(y1[j]<=t[i]){a=a+1}
  }
  x1[i]=a/n1
}

#n=1000时
n2=1000
x2=c(0,0,0,0,0,0,0,0,0)
y2=c(rnorm(n2,mean=0,sd=1))
for(i in 1:9){
  b=0
  for(j in 1:n2){
    if(y2[j]<=t[i]){b=b+1}
  }
  x2[i]=b/n2
}

#n=10000时
n3=10000
x3=c(0,0,0,0,0,0,0,0,0)
y3=c(rnorm(n3,mean=0,sd=1))
for(i in 1:9){
  c=0
  for(j in 1:n3){
    if(y3[j]<=t[i]){c=c+1}
  }
  x3[i]=c/n3
}
(table=data.frame(t,truevalue,x1,x2,x3))
```
 
####3.2 Draw box plots of the 100 approximation errors at each $t$ using **ggplot2** [@R-ggplot2] for each $n$.
 
 n=100，repeat the experiment 100 times.
 
```{r,echo=FALSE,results='hide'}
n=100
(x=matrix(0,100,9))
for (p in 1:100) {
  y=c(rnorm(n,mean=0,sd=1))
  for(i in 1:9){
    a=0
    for(j in 1:n){
      if(y[j]<=t[i]){a=a+1}
    }
    x[p,i]=a/n
  }
}
x
w=c(rep(truevalue[1],100),rep(truevalue[2],100),rep(truevalue[3],100),rep(truevalue[4],100),rep(truevalue[5],100),rep(truevalue[6],100),rep(truevalue[7],100),rep(truevalue[8],100),rep(truevalue[9],100))
m=matrix(w,nrow = 100,ncol = 9,byrow = FALSE)
(e=x-m)
E=as.vector(e)
T=c(rep(t[1],100),rep(t[2],100),rep(t[3],100),rep(t[4],100),rep(t[5],100),rep(t[6],100),rep(t[7],100),rep(t[8],100),rep(t[9],100))
(df=cbind(E,T))
(df=data.frame(df))
library(ggplot2)
ggplot(df,aes(T,E,group=T))+geom_boxplot()
```

 n=1000，repeat the experiment 100 times.

```{r,echo=FALSE,results='hide'}
n=1000
(x=matrix(0,100,9))
for (p in 1:100) {
  y=c(rnorm(n,mean=0,sd=1))
  for(i in 1:9){
    a=0
    for(j in 1:n){
      if(y[j]<=t[i]){a=a+1}
    }
    x[p,i]=a/n
  }
}
x
w=c(rep(truevalue[1],100),rep(truevalue[2],100),rep(truevalue[3],100),rep(truevalue[4],100),rep(truevalue[5],100),rep(truevalue[6],100),rep(truevalue[7],100),rep(truevalue[8],100),rep(truevalue[9],100))
m=matrix(w,nrow = 100,ncol = 9,byrow = FALSE)
(e=x-m)
E=as.vector(e)
T=c(rep(t[1],100),rep(t[2],100),rep(t[3],100),rep(t[4],100),rep(t[5],100),rep(t[6],100),rep(t[7],100),rep(t[8],100),rep(t[9],100))
(df=cbind(E,T))
(df=data.frame(df))
library(ggplot2)
ggplot(df,aes(T,E,group=T))+geom_boxplot()
```

n=10000，repeat the experiment 100 times.

```{r,echo=FALSE,results='hide'}
n=10000
(x=matrix(0,100,9))
for (p in 1:100) {
  y=c(rnorm(n,mean=0,sd=1))
  for(i in 1:9){
    a=0
    for(j in 1:n){
      if(y[j]<=t[i]){a=a+1}
    }
    x[p,i]=a/n
  }
}
x
w=c(rep(truevalue[1],100),rep(truevalue[2],100),rep(truevalue[3],100),rep(truevalue[4],100),rep(truevalue[5],100),rep(truevalue[6],100),rep(truevalue[7],100),rep(truevalue[8],100),rep(truevalue[9],100))
m=matrix(w,nrow = 100,ncol = 9,byrow = FALSE)
(e=x-m)
E=as.vector(e)
T=c(rep(t[1],100),rep(t[2],100),rep(t[3],100),rep(t[4],100),rep(t[5],100),rep(t[6],100),rep(t[7],100),rep(t[8],100),rep(t[9],100))
(df=cbind(E,T))
(df=data.frame(df))
library(ggplot2)
ggplot(df,aes(T,E,group=T))+geom_boxplot()
```
   
##4 Summeary and disscussion

   As the boxplot show ,with the increase of sample size n, the error of estimation decreases gradually.